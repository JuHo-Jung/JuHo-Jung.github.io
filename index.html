<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Juho Jung</title>

    <meta name="author" content="Juho Jung">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon-custom.jpeg" type="image/jpeg">
    <link rel="icon" href="images/favicon/favicon-custom.jpeg" type="image/jpeg">
    <link rel="apple-touch-icon" href="images/favicon/favicon-custom.jpeg">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
    <style>
      .pub { display:flex; align-items:center; gap:20px; margin:18px 0; }
      .pub .thumb { flex:0 0 300px; max-width:300px; }        /* 이미지 폭 고정 */
      .pub .thumb img{ width:100%; height:auto; object-fit:contain; display:block; }
      .pub .meta{ flex:1 1 auto; font-size:16px; line-height:1.5; }

      /* 전역 규칙에 눌리는 걸 방지 (단어마다 줄바꿈, 좁은 폭 등) */
      .pub .meta a, .pub .meta .papertitle{
        display:inline !important; 
        max-width:none !important; 
        white-space:normal !important;
        word-break:keep-all;        /* 한국어/영어 자연스런 줄바꿈 */
      }
      .pub .papertitle{ font-size:20px; font-weight:700; }
      .pub .links{ margin:4px 0 0; }
      
      /* Juho Jung author name styling */
      .pub .meta strong u { font-size: 1.1em; }
      @media (max-width:700px){
        .pub{ flex-direction:column; align-items:flex-start; }
        .pub .thumb{ width:100%; max-width:100%; }
      }
    </style>
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Juho Jung
                </p>
                <p>
		I am a Research Scientist (Alternative Military Service) at <a href="https://vuno.co/en/">VUNO Inc.</a> My research focuses on uncertainty aware multimodal LLMs adaptation in medical image and improving uncertainty quantification of LLMs and hallucinations in real world environments.
		</p>
		<p>
		Before joining VUNO I obtained my Master's from the Graduate School of Applied Artificial Intelligence at <a href="https://sw.skku.edu/eng_sw/index.do">SKKU</a> (Full Academic Excellence Scholarship), under the supervision of Prof. <a href="https://sites.google.com/site/jyhantop/jinyoung-han-ph-d">Jinyoung Han</a>, where I researched uncertainty-aware Multitask Learning in Medical image and Multimodal Feature Alignment in complex and unknown modality. I completed my bachelor's degree (Magna Cum Laude) in Applied Artificial Intelligence at the <a href="https://sw.skku.edu/eng_sw/index.do">SKKU</a>. I had the privilege of working with <a href="https://scholar.google.com/citations?user=uB2He2UAAAAJ&hl=en">Zhi-Qi Cheng</a>, <a href="https://scholar.google.co.uk/citations?user=Py54GcEAAAAJ&hl=en">Alexander Hauptmann</a>, and <a href="https://www.cs.cmu.edu/~dmortens/">David Mortensen</a> as a Visiting Scholar in the School of Computer Science Intensive Program in AI at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:zuho.jung@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/JuhoJung_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=YjI_sSAAAAAJ&hl=ko&oi=ao">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/JuHo-Jung">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/juhojung98">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/정주호_증명사진_002.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/정주호_증명사진_002.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research Interests</h2>
                <p>
                  I am interested in advancing uncertainty-aware multimodal learning for medical imaging. My work explores how to adapt large vision-language models to clinical data, quantify and calibrate their uncertainty, and mitigate hallucinations in real-world medical settings. My current research emphasizes:
                </p>
                <p>
                  • <strong>Vision-Language Models for medical imaging</strong><br>
                  • <strong>Uncertainty Quantification in multimodal learning</strong><br>
                  • <strong>Hallucination detection and mitigation</strong><br>
                  • <strong>Representation Learning for medical images</strong>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Selected Publications</h2>
                <p>
                  <em>(Equal contributions are denoted by *)</em>
                </p>
              </td>
            </tr>
          </tbody></table>
          
          <div class="pub">
            <div class="thumb">
              <a href="data/paper/logo-srrg-arxiv-2025.png">
                <img alt="paper logo" src="data/paper/logo-srrg-arxiv-2025.png">
              </a>
            </div>

            <div class="meta">
              <a href="https://huggingface.co/papers/2510.00428">
                <span class="papertitle">
                  Automated Structured Radiology Report Generation with Rich Clinical Context
                </span>
              </a><br>
              Seongjae Kang*, Dong Bok Lee*, <strong><u>Juho Jung</u></strong>, Dongseop Kim, Won Hwa Kim, Sunghoon Joo<br>
              <em>Under Review, arXiv</em>, 2025
              <div class="links">
                <a href="https://huggingface.co/papers/2510.00428">Project Page</a> /
                <a href="https://arxiv.org/pdf/2505.24223">arXiv</a> /
                <a href="data/paper/srrg-arxiv-2025.pdf">Paper</a>
              </div>
              <p style="margin-top:8px">
                Incorporating clinical context into automated structured radiology report generation improves report
                quality by addressing temporal hallucinations and utilizing comprehensive patient data.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/paper/logo-camel-wacv-2025.png">
                <img alt="paper logo" src="data/paper/logo-camel-wacv-2025.png">
              </a>
            </div>

            <div class="meta">
              <a href="https://openaccess.thecvf.com/content/WACV2025/html/Jung_CAMEL_Confidence-Aware_Multi-Task_Ensemble_Learning_with_Spatial_Information_for_Retina_WACV_2025_paper.html">
                <span class="papertitle">
                  CAMEL: Confidence-Aware Multi-Task Ensemble Learning with Spatial Information for Retina OCT Image Classification and Segmentation
                </span>
              </a><br>
              <strong><u>Juho Jung</u></strong>*, Migyeong Yang*, Hyunseon Won, Jiwon Kim, Jeong Mo Han, Joon Seo Hwang, Daniel Duck-Jin Hwang, Jinyoung Han<br>
              <em>WACV</em>, 2025
              <div class="links">
                <a href="https://openaccess.thecvf.com/content/WACV2025/html/Jung_CAMEL_Confidence-Aware_Multi-Task_Ensemble_Learning_with_Spatial_Information_for_Retina_WACV_2025_paper.html">Project Page</a> /
                <a href="data/paper/camel-wacv-2025.pdf">Paper</a>
              </div>
              <p style="margin-top:8px">
                A novel framework designed to reduce task-specific uncertainty in multi-task learning for retinal OCT image analysis through confidence-aware ensemble learning.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/paper/logo-hique-cikm-2025.png">
                <img alt="paper logo" src="data/paper/logo-hique-cikm-2025.png">
              </a>
            </div>

            <div class="meta">
              <a href="https://dl.acm.org/doi/abs/10.1145/3627673.3679797">
                <span class="papertitle">
                  HiQuE: Hierarchical Question Embedding Network for Multimodal Depression Detection
                </span>
              </a><br>
              <strong><u>Juho Jung</u></strong>, Chaewon Kang, Jeewoo Yoon, Seungbae Kim, Jinyoung Han<br>
              <em>CIKM</em>, 2024 (Oral)
              <div class="links">
                <a href="https://dl.acm.org/doi/abs/10.1145/3627673.3679797">Project Page</a> /
                <a href="data/paper/hique-cikm-2025.pdf">Paper</a>
              </div>
              <p style="margin-top:8px">
                A hierarchical question embedding network for multimodal depression detection using structured question-answer pairs.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/paper/logo-s6-arxiv-2024.png">
                <img alt="paper logo" src="data/paper/logo-s6-arxiv-2024.png">
              </a>
            </div>

            <div class="meta">
              <a href="https://arxiv.org/pdf/2407.13078">
                <span class="papertitle">
                  Enhancing Temporal Action Localization: Advanced s6 Modeling with Recurrent Mechanism
                </span>
              </a><br>
              Sangyoun Lee, <strong><u>Juho Jung</u></strong>, Changdae Oh, Sunghee Yun<br>
              <em>arXiv</em>, 2024
              <div class="links">
                <a href="https://arxiv.org/pdf/2407.13078">arXiv</a> /
                <a href="data/paper/s6-arxiv-2024.pdf">Paper</a>
              </div>
              <p style="margin-top:8px">
                Advanced s6 modeling with recurrent mechanism for enhanced temporal action localization.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/paper/logo-amd_prediction-scientificreport-2024.png">
                <img alt="paper logo" src="data/paper/logo-amd_prediction-scientificreport-2024.png">
              </a>
            </div>

            <div class="meta">
              <a href="data/paper/amd_prediction-scientificreport-2024.pdf">
                <span class="papertitle">
                  Prediction of neovascular age-related macular degeneration recurrence using optical coherence tomography images with a deep neural network
                </span>
              </a><br>
              <strong><u>Juho Jung</u></strong>, Jinyoung Han, Jeong Mo Han, Junseo Ko, Jeewoo Yoon, Joon Seo Hwang, Ji In Park, Gyudeok Hwang, Jae Ho Jung, Daniel Duck-Jin Hwang<br>
              <em>Scientific Reports</em>, 2024
              <div class="links">
                <a href="data/paper/amd_prediction-scientificreport-2024.pdf">Paper</a>
              </div>
              <p style="margin-top:8px">
                Deep neural network-based prediction of neovascular age-related macular degeneration recurrence using OCT images.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/paper/logo-safe-cikm-2024.png">
                <img alt="paper logo" src="data/paper/logo-safe-cikm-2024.png">
              </a>
            </div>

            <div class="meta">
              <a href="data/paper/safe-cikm-2024.pdf">
                <span class="papertitle">
                  SAFE: Sequential Attentive Face Embedding with Contrastive Learning for Deepfake Video Detection
                </span>
              </a><br>
              <strong><u>Juho Jung</u></strong>, Chaewon Kang, Jeewoo Yoon, Simon S Woo, Jinyoung Han<br>
              <em>CIKM</em>, 2024
              <div class="links">
                <a href="data/paper/safe-cikm-2024.pdf">Paper</a> /
                <a href="data/video/cikm_presentation_10.1145_3583780.3615279.mp4">video</a>
              </div>
              <p style="margin-top:8px">
                Sequential attentive face embedding with contrastive learning for deepfake video detection.
              </p>
            </div>
          </div>

          <p style="margin-top:20px; text-align:center;">
            <em>For more publications, please visit my <a href="https://scholar.google.com/citations?hl=ko&user=YjI_sSAAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> profile.</em>
          </p>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Projects</h2>
                <p>
                  <em>Project details will be added here as they become available.</em>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
