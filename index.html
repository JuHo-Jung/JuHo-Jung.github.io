<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Juho Jung</title>

    <meta name="author" content="Juho Jung">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Juho Jung
                </p>
                <p>
		I am a Research Scientist (Alternative Military Service) at <a href="https://vuno.co/en/">VUNO Inc.</a> My research focuses on uncertainty aware multimodal LLMs adaptation in medical image and improving uncertainty quantification of LLMs and hallucinations in real world environments.
		</p>
		<p>
		Before joining VUNO I obtained my Master's from the Graduate School of Applied Artificial Intelligence at <a href="https://sw.skku.edu/eng_sw/index.do">SKKU</a>, under the supervision of Prof. <a href="https://sites.google.com/site/jyhantop/jinyoung-han-ph-d">Jinyoung Han</a>, where I researched uncertainty-aware Multitask Learning in Medical image and Multimodal Feature Alignment in complex and unknown modality. I completed my bachelor's degree (Magna Cum Laude) in Applied Artificial Intelligence at the <a href="https://sw.skku.edu/eng_sw/index.do">SKKU</a>. I had the privilege of working with <a href="https://scholar.google.com/citations?user=uB2He2UAAAAJ&hl=en">Zhi-Qi Cheng</a>, <a href="https://scholar.google.co.uk/citations?user=Py54GcEAAAAJ&hl=en">Alexander Hauptmann</a>, and <a href="https://www.cs.cmu.edu/~dmortens/">David Mortensen</a> during visiting at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:zuho.jung@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/JuhoJung-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=YjI_sSAAAAAJ&hl=ko&oi=ao">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/JuHo-Jung">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/juhojung98">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/정주호_증명사진_001.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/정주호_증명사진_001.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research Interests</h2>
                <p>
                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about uncertainty quantification in multimodal learning, medical image analysis, and vision-language models. Some papers are <span class="highlight">highlighted</span>.
                </p>
                <p>
                  <strong>Vision Language Model</strong> • <strong>Uncertainty Quantification</strong> • <strong>Hallucinations</strong> • <strong>Representation Learning for Medical Image</strong>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Selected Publications</h2>
                <p>
                  <em>(Equal contributions are denoted by *)</em>
                </p>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <a href="data/paper/logo-srrg-arxiv-2025.png"><img style="width:160px;max-width:160px;object-fit: cover;" alt="paper logo" src="data/paper/logo-srrg-arxiv-2025.png"></a>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://huggingface.co/papers/2510.00428">
                  <span class="papertitle">Automated Structured Radiology Report Generation with Rich Clinical Context</span>
                </a>
                <br>
                <strong>Juho Jung</strong>*, Seongjae Kang*, Dong Bok Lee, Dongseop Kim, Won Hwa Kim, Sunghoon Joo
                <br>
                <em>arXiv</em>, 2025
                <br>
                <a href="https://huggingface.co/papers/2510.00428">project page</a> /
                <a href="https://arxiv.org/pdf/2505.24223">arXiv</a> /
                <a href="data/paper/srrg-arxiv-2025.pdf">paper</a>
                <p></p>
                <p>
                  Incorporating clinical context into automated structured radiology report generation improves report quality by addressing temporal hallucinations and utilizing comprehensive patient data.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <a href="data/paper/logo-camel-wacv-2025.png"><img style="width:160px;max-width:160px;object-fit: cover;" alt="paper logo" src="data/paper/logo-camel-wacv-2025.png"></a>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/WACV2025/html/Jung_CAMEL_Confidence-Aware_Multi-Task_Ensemble_Learning_with_Spatial_Information_for_Retina_WACV_2025_paper.html">
                  <span class="papertitle">CAMEL: Confidence-Aware Multi-Task Ensemble Learning with Spatial Information for Retina OCT Image Classification and Segmentation</span>
                </a>
                <br>
                <strong>Juho Jung</strong>, Migyeong Yang, Hyunseon Won, Jiwon Kim, Jeong Mo Han, Joon Seo Hwang, Daniel Duck-Jin Hwang, Jinyoung Han
                <br>
                <em>WACV</em>, 2025
                <br>
                <a href="https://openaccess.thecvf.com/content/WACV2025/html/Jung_CAMEL_Confidence-Aware_Multi-Task_Ensemble_Learning_with_Spatial_Information_for_Retina_WACV_2025_paper.html">project page</a> /
                <a href="data/paper/camel-wacv-2025.pdf">paper</a>
                <p></p>
                <p>
                  A novel framework designed to reduce task-specific uncertainty in multi-task learning for retinal OCT image analysis through confidence-aware ensemble learning.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <a href="data/paper/logo-hique-cikm-2025.png"><img style="width:160px;max-width:160px;object-fit: cover;" alt="paper logo" src="data/paper/logo-hique-cikm-2025.png"></a>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3627673.3679797">
                  <span class="papertitle">HiQuE: Hierarchical Question Embedding Network for Multimodal Depression Detection</span>
                </a>
                <br>
                <strong>Juho Jung</strong>*, Seongjae Kang*, Dong Bok Lee, Dongseop Kim, Won Hwa Kim, Sunghoon Joo
                <br>
                <em>CIKM</em>, 2024 (Oral)
                <br>
                <a href="https://dl.acm.org/doi/abs/10.1145/3627673.3679797">project page</a> /
                <a href="data/paper/hique-cikm-2025.pdf">paper</a>
                <p></p>
                <p>
                  A hierarchical question embedding network for multimodal depression detection using structured question-answer pairs.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <a href="data/paper/logo-s6-arxiv-2024.png"><img style="width:160px;max-width:160px;object-fit: cover;" alt="paper logo" src="data/paper/logo-s6-arxiv-2024.png"></a>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2407.13078">
                  <span class="papertitle">Enhancing Temporal Action Localization: Advanced s6 Modeling with Recurrent Mechanism</span>
                </a>
                <br>
                <strong>Juho Jung</strong>
                <br>
                <em>arXiv</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2407.13078">arXiv</a> /
                <a href="data/paper/s6-arxiv-2024.pdf">paper</a>
                <p></p>
                <p>
                  Advanced s6 modeling with recurrent mechanism for enhanced temporal action localization.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <a href="data/paper/logo-amd_prediction-scientificreport-2024.png"><img style="width:160px;max-width:160px;object-fit: cover;" alt="paper logo" src="data/paper/logo-amd_prediction-scientificreport-2024.png"></a>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="data/paper/amd_prediction-scientificreport-2024.pdf">
                  <span class="papertitle">Prediction of neovascular age-related macular degeneration recurrence using optical coherence tomography images with a deep neural network</span>
                </a>
                <br>
                <strong>Juho Jung</strong>*, Jiwon Kim*, Joon Seo Hwang, Daniel Duck-Jin Hwang, Jinyoung Han
                <br>
                <em>Scientific Reports</em>, 2024
                <br>
                <a href="data/paper/amd_prediction-scientificreport-2024.pdf">paper</a>
                <p></p>
                <p>
                  Deep neural network-based prediction of neovascular age-related macular degeneration recurrence using OCT images.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <a href="data/paper/logo-safe-cikm-2024.png"><img style="width:160px;max-width:160px;object-fit: cover;" alt="paper logo" src="data/paper/logo-safe-cikm-2024.png"></a>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="data/paper/safe-cikm-2024.pdf">
                  <span class="papertitle">SAFE: Sequential Attentive Face Embedding with Contrastive Learning for Deepfake Video Detection</span>
                </a>
                <br>
                <strong>Juho Jung</strong>*, Seongjae Kang*, Dong Bok Lee, Dongseop Kim, Won Hwa Kim, Sunghoon Joo
                <br>
                <em>CIKM</em>, 2024
                <br>
                <a href="data/paper/safe-cikm-2024.pdf">paper</a> /
                <a href="data/video/cikm_presentation_10.1145_3583780.3615279.mp4">video</a>
                <p></p>
                <p>
                  Sequential attentive face embedding with contrastive learning for deepfake video detection.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:8px;width:100%;vertical-align:middle">
                <p>
                  <em>For more publications, please visit my <a href="https://scholar.google.com/citations?hl=ko&user=YjI_sSAAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> profile.</em>
                </p>
              </td>
            </tr>
            
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Projects</h2>
                <p>
                  <em>Project details will be added here as they become available.</em>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
