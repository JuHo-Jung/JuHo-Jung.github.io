<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Juho Jung</title>

    <meta name="author" content="Juho Jung">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/정주호_증명사진_001.jpeg?v=2" type="image/jpeg">
    <link rel="icon" href="images/favicon/정주호_증명사진_001.jpeg?v=2" type="image/jpeg">
    <link rel="apple-touch-icon" href="images/favicon/정주호_증명사진_001.jpeg?v=2">
    <link rel="icon" type="image/png" sizes="32x32" href="images/favicon/정주호_증명사진_001.jpeg?v=2">
    <link rel="icon" type="image/png" sizes="16x16" href="images/favicon/정주호_증명사진_001.jpeg?v=2">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
    <style>
      /* Jon Barron style - simple and clean */
      .pub { display:flex; align-items:center; gap:20px; margin:18px 0; }
      .pub .thumb { flex:0 0 250px; max-width:250px; }
      .pub .thumb img{ width:100%; height:180px; object-fit:contain; display:block; }
      .pub .meta{ flex:1 1 auto; font-size:16px; line-height:1.5; }
      .pub .papertitle{ font-size:18px; font-weight:700; }
      .pub .links{ margin:4px 0 0; }
      
      /* Juho Jung author name styling */
      .pub .meta strong u { font-size: 1.1em; }
      
      /* Mobile responsive improvements */
      @media (max-width: 768px) {
        /* Main container adjustments */
        table[style*="max-width:800px"] {
          max-width: 100% !important;
          padding: 10px !important;
        }
        
        /* Profile section: stack vertically on mobile */
        table[style*="width:63%"] { 
          width: 100% !important; 
          display: block !important; 
          padding: 10px !important;
        }
        table[style*="width:37%"] { 
          width: 100% !important; 
          display: block !important; 
          text-align: center;
          padding: 10px !important;
        }
        
        /* Paper section: stack vertically on mobile */
        .pub { 
          flex-direction: column !important; 
          align-items: flex-start !important; 
          gap: 15px !important;
        }
        .pub .thumb { 
          width: 100% !important; 
          max-width: 100% !important; 
          flex: none !important; 
        }
        
        /* Image scaling for mobile */
        img { 
          max-width: 100% !important; 
          height: auto !important; 
        }
        
        /* Text size adjustments */
        .pub .papertitle { font-size: 16px !important; }
        .pub .meta { font-size: 14px !important; }
        
        /* Profile image sizing */
        table[style*="width:37%"] img {
          max-width: 200px !important;
          width: auto !important;
        }
      }
      
      /* Very small screens */
      @media (max-width: 480px) {
        .pub .papertitle { font-size: 14px !important; }
        .pub .meta { font-size: 13px !important; }
        table[style*="width:37%"] img {
          max-width: 150px !important;
        }
      }
    </style>
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Juho Jung
                </p>
                <p>
		I am a Research Scientist (Alternative Military Service) at <a href="https://vuno.co/en/">VUNO Inc.</a> My research focuses on uncertainty aware multimodal LLMs adaptation in medical image and improving uncertainty quantification of LLMs and hallucinations in real world environments.
		</p>
		<p>
		Before joining VUNO I obtained my Master's from the Graduate School of Applied Artificial Intelligence at <a href="https://sw.skku.edu/eng_sw/index.do">SKKU</a> (Full Academic Excellence Scholarship), under the supervision of Prof. <a href="https://sites.google.com/site/jyhantop/jinyoung-han-ph-d">Jinyoung Han</a>, where I researched uncertainty-aware Multitask Learning in Medical image and Multimodal Feature Alignment in complex and unknown modality. I completed my bachelor's degree (Magna Cum Laude) in Applied Artificial Intelligence at the <a href="https://sw.skku.edu/eng_sw/index.do">SKKU</a>. I had the privilege of working with <a href="https://scholar.google.com/citations?user=uB2He2UAAAAJ&hl=en">Zhi-Qi Cheng</a>, <a href="https://scholar.google.co.uk/citations?user=Py54GcEAAAAJ&hl=en">Alexander Hauptmann</a>, and <a href="https://www.cs.cmu.edu/~dmortens/">David Mortensen</a> as a Visiting Scholar in the School of Computer Science Intensive Program in AI at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:zuho.jung@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/JuhoJung_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=YjI_sSAAAAAJ&hl=ko&oi=ao">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/JuHo-Jung">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/juhojung98">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/정주호_증명사진_002.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/정주호_증명사진_002.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research Interests</h2>
                <p>
                  I am interested in advancing uncertainty-aware multimodal learning for medical imaging. My work explores how to adapt large vision-language models to clinical data, quantify and calibrate their uncertainty, and mitigate hallucinations in real-world medical settings. My current research emphasizes:
                </p>
                <p>
                  • <strong>Vision-Language Models for medical imaging</strong><br>
                  • <strong>Uncertainty Quantification in multimodal learning</strong><br>
                  • <strong>Hallucination detection and mitigation</strong><br>
                  • <strong>Representation Learning for medical images</strong>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Selected Publications</h2>
                <p>
                  <em>(Equal contributions are denoted by *)</em>
                </p>
              </td>
            </tr>
          </tbody></table>
          
          <div class="pub">
            <div class="thumb">
              <a href="data/paper/logo-srrg-arxiv-2025.png">
                <img alt="paper logo" src="data/paper/logo-srrg-arxiv-2025.png">
              </a>
            </div>

            <div class="meta">
              <a href="https://huggingface.co/papers/2510.00428">
                <span class="papertitle">
                  Automated Structured Radiology Report Generation with Rich Clinical Context
                </span>
              </a><br>
              Seongjae Kang*, Dong Bok Lee*, <strong><u>Juho Jung</u></strong>, Dongseop Kim, Won Hwa Kim, Sunghoon Joo<br>
              <em>Under Review, arXiv</em>, 2025
              <div class="links">
                <a href="https://huggingface.co/papers/2510.00428">Project Page</a> /
                <a href="https://arxiv.org/pdf/2505.24223">arXiv</a> /
                <a href="data/paper/srrg-arxiv-2025.pdf">Paper</a>
              </div>
              <p style="margin-top:8px">
                Incorporating clinical context into automated structured radiology report generation improves report
                quality by addressing temporal hallucinations and utilizing comprehensive patient data.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/paper/logo-camel-wacv-2025.png">
                <img alt="paper logo" src="data/paper/logo-camel-wacv-2025.png">
              </a>
            </div>

            <div class="meta">
              <a href="https://openaccess.thecvf.com/content/WACV2025/html/Jung_CAMEL_Confidence-Aware_Multi-Task_Ensemble_Learning_with_Spatial_Information_for_Retina_WACV_2025_paper.html">
                <span class="papertitle">
                  CAMEL: Confidence-Aware Multi-Task Ensemble Learning with Spatial Information for Retina OCT Image Classification and Segmentation
                </span>
              </a><br>
              <strong><u>Juho Jung</u></strong>*, Migyeong Yang*, Hyunseon Won, Jiwon Kim, Jeong Mo Han, Joon Seo Hwang, Daniel Duck-Jin Hwang, Jinyoung Han<br>
              <em>WACV</em>, 2025
              <div class="links">
                <a href="https://openaccess.thecvf.com/content/WACV2025/html/Jung_CAMEL_Confidence-Aware_Multi-Task_Ensemble_Learning_with_Spatial_Information_for_Retina_WACV_2025_paper.html">Project Page</a> /
                <a href="data/paper/camel-wacv-2025.pdf">Paper</a>
              </div>
              <p style="margin-top:8px">
                A novel framework designed to reduce task-specific uncertainty in multi-task learning for retinal OCT image analysis through confidence-aware ensemble learning.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/paper/logo-hique-cikm-2025.png">
                <img alt="paper logo" src="data/paper/logo-hique-cikm-2025.png">
              </a>
            </div>

            <div class="meta">
              <a href="https://dl.acm.org/doi/abs/10.1145/3627673.3679797">
                <span class="papertitle">
                  HiQuE: Hierarchical Question Embedding Network for Multimodal Depression Detection
                </span>
              </a><br>
              <strong><u>Juho Jung</u></strong>, Chaewon Kang, Jeewoo Yoon, Seungbae Kim, Jinyoung Han<br>
              <em>CIKM</em>, 2024 (Oral)
              <div class="links">
                <a href="https://dl.acm.org/doi/abs/10.1145/3627673.3679797">Project Page</a> /
                <a href="data/paper/hique-cikm-2025.pdf">Paper</a>
              </div>
              <p style="margin-top:8px">
                A hierarchical question embedding network for multimodal depression detection using structured question-answer pairs.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/paper/logo-s6-arxiv-2024.png">
                <img alt="paper logo" src="data/paper/logo-s6-arxiv-2024.png">
              </a>
            </div>

            <div class="meta">
              <a href="https://arxiv.org/pdf/2407.13078">
                <span class="papertitle">
                  Enhancing Temporal Action Localization: Advanced s6 Modeling with Recurrent Mechanism
                </span>
              </a><br>
              Sangyoun Lee, <strong><u>Juho Jung</u></strong>, Changdae Oh, Sunghee Yun<br>
              <em>arXiv</em>, 2024
              <div class="links">
                <a href="https://arxiv.org/pdf/2407.13078">arXiv</a> /
                <a href="data/paper/s6-arxiv-2024.pdf">Paper</a>
              </div>
              <p style="margin-top:8px">
                Advanced s6 modeling with recurrent mechanism for enhanced temporal action localization.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/paper/logo-amd_prediction-scientificreport-2024.png">
                <img alt="paper logo" src="data/paper/logo-amd_prediction-scientificreport-2024.png">
              </a>
            </div>

            <div class="meta">
              <a href="data/paper/amd_prediction-scientificreport-2024.pdf">
                <span class="papertitle">
                  Prediction of neovascular age-related macular degeneration recurrence using optical coherence tomography images with a deep neural network
                </span>
              </a><br>
              <strong><u>Juho Jung</u></strong>, Jinyoung Han, Jeong Mo Han, Junseo Ko, Jeewoo Yoon, Joon Seo Hwang, Ji In Park, Gyudeok Hwang, Jae Ho Jung, Daniel Duck-Jin Hwang<br>
              <em>Scientific Reports</em>, 2024
              <div class="links">
                <a href="data/paper/amd_prediction-scientificreport-2024.pdf">Paper</a>
              </div>
              <p style="margin-top:8px">
                Deep neural network-based prediction of neovascular age-related macular degeneration recurrence using OCT images.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/paper/logo-safe-cikm-2024.png">
                <img alt="paper logo" src="data/paper/logo-safe-cikm-2024.png">
              </a>
            </div>

            <div class="meta">
              <a href="data/paper/safe-cikm-2024.pdf">
                <span class="papertitle">
                  SAFE: Sequential Attentive Face Embedding with Contrastive Learning for Deepfake Video Detection
                </span>
              </a><br>
              <strong><u>Juho Jung</u></strong>, Chaewon Kang, Jeewoo Yoon, Simon S Woo, Jinyoung Han<br>
              <em>CIKM</em>, 2024
              <div class="links">
                <a href="data/paper/safe-cikm-2024.pdf">Paper</a> /
                <a href="data/video/cikm_presentation_10.1145_3583780.3615279.mp4">video</a>
              </div>
              <p style="margin-top:8px">
                Sequential attentive face embedding with contrastive learning for deepfake video detection.
              </p>
            </div>
          </div>

          <p style="margin-top:20px; text-align:center;">
            <em>For more publications, please visit my <a href="https://scholar.google.com/citations?hl=ko&user=YjI_sSAAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> profile.</em>
          </p>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Selected Projects</h2>
              </td>
            </tr>
          </tbody></table>
          
          <div class="pub">
            <div class="thumb">
              <a href="data/project/logo-luna25.png">
                <img alt="project logo" src="data/project/logo-luna25.png">
              </a>
            </div>

            <div class="meta">
              <a href="https://luna25.grand-challenge.org/ai-phases-rules/">
                <span class="papertitle">
                  LUNA25 Challenge
                </span>
              </a><br>
              <em>March 2025 - September 2025</em><br>
              <div class="links">
                <a href="https://luna25.grand-challenge.org/ai-phases-rules/">Project Page</a> /
                <a href="data/project/LUNA_25.pdf">Paper</a>
              </div>
              <p style="margin-top:8px">
                Participating in the LUNA25 AI Challenge for lung nodule analysis and detection using advanced deep learning techniques.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/project/logo-www.png">
                <img alt="project logo" src="data/project/logo-www.png">
              </a>
            </div>

            <div class="meta">
              <a href="data/project/WWW.pdf">
                <span class="papertitle">
                  Explainable AI for Audio-Visual Deepfake Video Detection
                </span>
              </a><br>
              <em>Carnegie Mellon University</em>, <em>Institute of Information & communications Technology Planning & Evaluation (IITP)</em><br>
              <em>September 2023 - February 2024</em><br>
              <div class="links">
                <a href="data/project/WWW.pdf">Paper</a>
              </div>
              <p style="margin-top:8px">
                Developed explainable AI models for detecting deepfake videos using audio-visual features with attention mechanisms and gradient-based explanations.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/project/logo-GraphEHR.png">
                <img alt="project logo" src="data/project/logo-GraphEHR.png">
              </a>
            </div>

            <div class="meta">
              <a href="https://github.com/Juho-Jung/MIMIC_GNN?tab=readme-ov-file">
                <span class="papertitle">
                  Heterogeneous Graph Neural Network for Electronic Health Records
                </span>
              </a><br>
              <em>Carnegie Mellon University</em>, <em>Institute of Information & communications Technology Planning & Evaluation (IITP)</em><br>
              <em>September 2023 - December 2023</em><br>
              <div class="links">
                <a href="data/project/GraphEHR.pdf">Paper</a> /
                <a href="https://github.com/Juho-Jung/MIMIC_GNN?tab=readme-ov-file">Code</a>
              </div>
              <p style="margin-top:8px">
                Implemented heterogeneous graph neural networks for modeling complex relationships in electronic health records using MIMIC dataset.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/project/logo-oct.png">
                <img alt="project logo" src="data/project/logo-oct.png">
              </a>
            </div>

            <div class="meta">
              <a href="data/project/Anti-VEGFtreatment.pdf">
                <span class="papertitle">
                  Clinical Decision Support System for Retinal Disease Detection with Explainable AI
                </span>
              </a><br>
              <em>National Research Foundation of Korea (KRF)</em><br>
              <em>March 2023 - February 2024</em><br>
              <div class="links">
                <a href="data/project/Anti-VEGFtreatment.pdf">Paper 1</a> /
                <a href="data/project/recurrence.pdf">Paper 2</a>
              </div>
              <p style="margin-top:8px">
                Developed explainable AI systems for retinal disease detection using OCT images with clinical decision support and recurrence prediction capabilities.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/project/logo-asoml.png">
                <img alt="project logo" src="data/project/logo-asoml.png">
              </a>
            </div>

            <div class="meta">
              <a href="data/project/ISOML.pdf">
                <span class="papertitle">
                  Study on Self-Driving B5G Networks towards Federated Private-5G
                </span>
              </a><br>
              <em>National Research Foundation (NRF) of Korea Grant funded by the Korean Government (MSIT)</em><br>
              <em>June 2021 - August 2024</em><br>
              <div class="links">
                <a href="data/project/ISOML.pdf">Paper</a>
              </div>
              <p style="margin-top:8px">
                Researched self-driving networks and federated learning approaches for private 5G networks with autonomous network management capabilities.
              </p>
            </div>
          </div>

          <div class="pub">
            <div class="thumb">
              <a href="data/project/logo-lifet.png">
                <img alt="project logo" src="data/project/logo-lifet.png">
              </a>
            </div>

            <div class="meta">
              <a href="https://lifet.co.kr/Survey/AI/Intro">
                <span class="papertitle">
                  Developing Pet Disease Detection Model and LLMs for Community
                </span>
              </a><br>
              <em>Lifet</em>, <em>February 2022 - September 2023</em><br>
              <div class="links">
                <a href="https://lifet.co.kr/Survey/AI/Intro">Website</a>
              </div>
              <p style="margin-top:8px">
                Developed an AI model for single-image disease detection and segmentation, deployed and optimized AI models in web, mobile, and LLM adaptation with customer community.
              </p>
            </div>
          </div>

          <p style="margin-top:20px; text-align:center;">
            <em>For more projects, please see my <a href="data/JuhoJung_CV.pdf">CV</a>.</em>
          </p>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Template by <a href="https://jonbarron.info/">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
